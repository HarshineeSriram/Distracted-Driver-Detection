{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = 'filtered images/c'\n",
    "class_values2=[]\n",
    "images2 = [] \n",
    "for i in range(10):\n",
    "    for root, dirs, files in os.walk(\"filtered images/c\"+str(i)):  \n",
    "        for filename in files[:10]:\n",
    "            im_path = 'filtered images/c' + str(i) + '/' + str(filename)\n",
    "            img = cv2.imread('filtered images/c' + str(i) + '/' + str(filename))\n",
    "            img = cv2.resize(img, (100,100))/256\n",
    "            images2.append(img)\n",
    "            class_values2.append(i)\n",
    "np_images2 = np.asarray(images2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.asarray(class_values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten\n",
    "model_base = keras.applications.vgg16.VGG16(include_top=False, input_shape=(100, 100,3), weights='imagenet')\n",
    "output = model_base.output\n",
    "output = Flatten()(output)\n",
    "output = Dense(256, activation = 'sigmoid')(output)\n",
    "output = Dropout(0.4)(output)\n",
    "output = Dense(64, activation = 'sigmoid')(output)\n",
    "output = Dropout(0.4)(output)\n",
    "output = Dense(10, activation = 'softmax')(output)\n",
    "\n",
    "model = Model(model_base.input, output)\n",
    "for layer in model_base.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/5\n",
      "90/90 [==============================] - 5s 52ms/step - loss: 2.5468 - acc: 0.0889 - val_loss: 3.2901 - val_acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 4s 42ms/step - loss: 2.3924 - acc: 0.1000 - val_loss: 3.4154 - val_acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 4s 43ms/step - loss: 2.3303 - acc: 0.1222 - val_loss: 3.5407 - val_acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 4s 42ms/step - loss: 2.2020 - acc: 0.1222 - val_loss: 3.6454 - val_acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 4s 43ms/step - loss: 2.3950 - acc: 0.1111 - val_loss: 3.6967 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bab58c8ba8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten\n",
    "model_base = keras.applications.vgg16.VGG16(include_top=False, input_shape=(100, 100,3), weights='imagenet')\n",
    "output = model_base.output\n",
    "output = Flatten()(output)\n",
    "output = Dense(256, activation = 'sigmoid')(output)\n",
    "output = Dropout(0.4)(output)\n",
    "output = Dense(64, activation = 'sigmoid')(output)\n",
    "output = Dropout(0.4)(output)\n",
    "output = Dense(10, activation = 'softmax')(output)\n",
    "\n",
    "model = Model(model_base.input, output)\n",
    "for layer in model_base.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.compile(optimizer = 'adam' , loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(np_images2, targets, epochs = 5, validation_split = 0.1, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    img = cv2.resize(img, (100,100))\n",
    "    img = img.reshape((-1,100,100,3))\n",
    "    class_value = model.predict(img)\n",
    "    return class_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13117987"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('filtered images/c3/img_100419.jpg')\n",
    "predict(img).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret==False:\n",
    "        continue\n",
    "    #frame = imutils.resize(frame, width=500)\n",
    "    #frame1 = frame\n",
    "    cv2.imshow(\"Face\", frame)\n",
    "    #gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    class_val = predict(frame)\n",
    "    cv2.rectangle(frame, (50,50), (100,100), (255,255,255), 2)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.load(open('knn classifier.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-672fc3d718b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'svm classifier.sav'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "pickle.load(open('svm classifier.sav', 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
